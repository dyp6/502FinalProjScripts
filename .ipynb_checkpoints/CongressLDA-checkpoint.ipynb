{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('rt')\n",
    "eng_stopwords.append('qt')\n",
    "eng_stopwords.append('&amp')\n",
    "eng_stopwords.append('amp')\n",
    "eng_stopwords.append('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text_no_links') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, monotonically_increasing_id, col, when\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.clustering import LDA, LDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/June2017.csv/*.part\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = congDF.drop(\"_c0\")\n",
    "data = congDF.filter(congDF['text'].isNull()==False)\n",
    "\n",
    "#data.select('text').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noLinks = data.withColumn('index',monotonically_increasing_id())\n",
    "noLinks = noLinks.withColumn('text_no_links',regexp_replace('text','http.*($|\\s)',''))\n",
    "\n",
    "#noLinks.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text with the pipeline\n",
    "congress = pipeline.fit(noLinks).transform(noLinks)\n",
    "#congress.select('finished_clean_lemma').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = congress.select('finished_clean_lemma').withColumn('index',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "cv = CountVectorizer(inputCol=\"finished_clean_lemma\",outputCol=\"features\",\n",
    "                     vocabSize=3500,minDF = 8.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF\n",
    "cvmodel = cv.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "result_cv = cvmodel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=3\n",
    "max_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(k=num_topics,maxIter=max_iter,optimizer='online').fit(result_cv.select(\"index\",\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = lda_model.transform(result_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTopDist(row):\n",
    "    return row.topicDistribution.toArray().tolist()\n",
    "DF = transform.rdd.map(extractTopDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = spark.createDataFrame(DF,[\"Topic1\",\"Topic2\",\"Topic3\"]).withColumn(\"index_1\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoTopDF = transform.join(DF,transform.index==DF.index_1,'inner')\\\n",
    ".select([\"index\",\"finished_clean_lemma\",\"features\",\"Topic1\",\"Topic2\",\"Topic3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CongressTopics = CoTopDF.rdd.map(lambda r: r.asDict())\\\n",
    "       .map(lambda r: Row(MaxTopic=[max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                        key=lambda kv: kv[1])[0],\n",
    "                                   max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                      key=lambda kv: kv[1])[1]], **r) )\\\n",
    "       .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsCongress = CongressTopics.withColumn(\"tweet_content\",col(\"finished_clean_lemma\"))\\\n",
    ".withColumn(\"Idx\",col(\"index\"))\\\n",
    ".drop(\"finished_clean_lemma\").drop(\"index\")\\\n",
    ".drop(\"Topic1\").drop(\"Topic2\").drop(\"Topic3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TopicsCongress.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices       |termWeights                                                                                                   |\n",
      "+-----+------------------+--------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[1, 0, 2, 3, 5]   |[0.028399792264002622, 0.027505717915976, 0.025889878035053358, 0.024564169627953828, 0.01801591830408264]    |\n",
      "|1    |[10, 35, 40, 4, 0]|[0.01360761939099871, 0.011240882646675708, 0.010539315120029126, 0.010502358277515673, 0.009872355136037902] |\n",
      "|2    |[9, 4, 15, 25, 27]|[0.013221634343320065, 0.013052844421598504, 0.011492108564766785, 0.009220411810831662, 0.009127870868404582]|\n",
      "+-----+------------------+--------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model.describeTopics(5).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|topicDistribution                                             |\n",
      "+--------------------------------------------------------------+\n",
      "|[0.7989402834060485,0.02524204271710266,0.17581767387684885]  |\n",
      "|[0.019143224281580586,0.8948697457154637,0.0859870300029557]  |\n",
      "|[0.9639917256201228,0.014853243126906527,0.021155031252970722]|\n",
      "|[0.4015449790166039,0.02173458305284354,0.5767204379305526]   |\n",
      "|[0.0430605522251171,0.5045839491961142,0.4523554985787687]    |\n",
      "|[0.7565930029387901,0.22304899926694674,0.020357997794263286] |\n",
      "|[0.6373857192880207,0.02574980565361803,0.3368644750583614]   |\n",
      "|[0.034174997619897604,0.4573142754573956,0.5085107269227068]  |\n",
      "|[0.40501261179828946,0.5596651107310903,0.03532227747062027]  |\n",
      "|[0.0270401114004059,0.3925295678498196,0.5804303207497745]    |\n",
      "+--------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform.select(\"topicDistribution\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3196, 3, [2029.0923, 2095.048, 1909.8921, 1812.0948, 104.6979, 1329.0313, 1079.231, 1080.4057, ..., 0.5127, 7.7615, 0.4548, 0.6301, 2.4416, 8.7418, 4.3756, 3.0019], 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvRDD = result_cv.select(\"features\",\"finished_clean_lemma\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvRDD.map(lambda r: Row(Term = [i for i in r[0][1]],Idx = [i for i in r[0][0].indices]))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   1,    2,   20,   27,  124,  255,  285, 1680], dtype=int32)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvRDD.map(lambda r: r[0].indices).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listen']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvRDD.map(lambda r: r[1][0]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeatDF.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extractFeats(row):\n",
    "   #return row.features.toArray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeatMatrix = result_cv.rdd.map(extractFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FeatDF = FeatMatrix.toDF().withColumn(\"index_2\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Terms = cvmodel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
