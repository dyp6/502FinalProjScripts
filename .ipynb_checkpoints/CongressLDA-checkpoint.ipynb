{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('rt')\n",
    "eng_stopwords.append('qt')\n",
    "eng_stopwords.append('&amp')\n",
    "eng_stopwords.append('amp')\n",
    "eng_stopwords.append('+')\n",
    "eng_stopwords.append('w')\n",
    "eng_stopwords.append('today')\n",
    "eng_stopwords.append('live')\n",
    "eng_stopwords.append('make')\n",
    "eng_stopwords.append('hear')\n",
    "eng_stopwords.append('meet')\n",
    "eng_stopwords.append('thank')\n",
    "eng_stopwords.append('see')\n",
    "eng_stopwords.append('time')\n",
    "eng_stopwords.append('day')\n",
    "eng_stopwords.append('watch')\n",
    "eng_stopwords.append('get')\n",
    "eng_stopwords.append('th')\n",
    "eng_stopwords.append('year')\n",
    "eng_stopwords.append('la')\n",
    "eng_stopwords.append('pm')\n",
    "eng_stopwords.append('hr')\n",
    "eng_stopwords.append('rep')\n",
    "eng_stopwords.append('come')\n",
    "eng_stopwords.append('last')\n",
    "eng_stopwords.append('dc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as f\n",
    "import boto3, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text_no_links') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse vector of counts for LDA input featuresCol\n",
    "cv = CountVectorizer(inputCol=\"finished_clean_lemma\",\n",
    "                     outputCol=\"features\", vocabSize=3500,\n",
    "                     minDF=8)\n",
    "# IDF to prepare for LDA\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "# LDA model\n",
    "lda = LDA(k=4, maxIter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark-NLP Pipeline\n",
    "mlPipeline = Pipeline(stages=[\n",
    "    cv,\n",
    "    idf,\n",
    "    lda\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF0617 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/June2017.csv/*.part\",header=True)\n",
    "congDF0717 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/July2017.csv/*.part\",header=True)\n",
    "congDF0817 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Aug2017.csv/*.part\",header=True)\n",
    "congDF0917 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Sep2017.csv/*.part\",header=True)\n",
    "congDF1017 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Oct2017.csv/*.part\",header=True)\n",
    "congDF1117 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Nov2017.csv/*.part\",header=True)\n",
    "congDF1217 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Dec2017.csv/*.part\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF17 = sc.union([congDF0617.rdd,congDF0717.rdd,\n",
    "                  congDF0817.rdd,congDF0917.rdd,\n",
    "                  congDF1017.rdd,congDF1117.rdd,\n",
    "                  congDF1217.rdd]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF0118 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Jan2018.csv/*.part\",header=True)\n",
    "congDF0218 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Feb2018.csv/*.part\",header=True)\n",
    "congDF0318 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Mar2018.csv/*.part\",header=True)\n",
    "congDF0418 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Apr2018.csv/*.part\",header=True)\n",
    "congDF0518 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/May2018.csv/*.part\",header=True)\n",
    "congDF0618 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/June2018.csv/*.part\",header=True)\n",
    "congDF0718 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/July2018.csv/*.part\",header=True)\n",
    "congDF0818 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Aug2018.csv/*.part\",header=True)\n",
    "congDF0918 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Sep2018.csv/*.part\",header=True)\n",
    "congDF1018 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Oct2018.csv/*.part\",header=True)\n",
    "congDF1118 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Nov2018.csv/*.part\",header=True)\n",
    "congDF1218 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Dec2018.csv/*.part\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF18 = sc.union([congDF0118.rdd,congDF0218.rdd,\n",
    "                     congDF0318.rdd,congDF0418.rdd,\n",
    "                     congDF0518.rdd,congDF0618.rdd,\n",
    "                     congDF0718.rdd,congDF0818.rdd,\n",
    "                     congDF0918.rdd,congDF1018.rdd,\n",
    "                     congDF1118.rdd,congDF1218.rdd]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF0119 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Jan2019.csv/*.part\",header=True)\n",
    "congDF0219 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Feb2019.csv/*.part\",header=True)\n",
    "congDF0319 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Mar2019.csv/*.part\",header=True)\n",
    "congDF0419 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Apr2019.csv/*.part\",header=True)\n",
    "congDF0519 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/May2019.csv/*.part\",header=True)\n",
    "congDF0619 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/June2019.csv/*.part\",header=True)\n",
    "congDF0719 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/July2019.csv/*.part\",header=True)\n",
    "congDF0819 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Aug2019.csv/*.part\",header=True)\n",
    "congDF0919 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Sep2019.csv/*.part\",header=True)\n",
    "congDF1019 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Oct2019.csv/*.part\",header=True)\n",
    "congDF1119 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Nov2019.csv/*.part\",header=True)\n",
    "congDF1219 = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/Dec2019.csv/*.part\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF19 = sc.union([congDF0119.rdd,congDF0219.rdd,\n",
    "                     congDF0319.rdd,congDF0419.rdd,\n",
    "                     congDF0519.rdd,congDF0619.rdd,\n",
    "                     congDF0719.rdd,congDF0819.rdd,\n",
    "                     congDF0919.rdd,congDF1019.rdd,\n",
    "                     congDF1119.rdd,congDF1219.rdd]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = sc.union([congDF17.rdd,congDF18.rdd,congDF19.rdd]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Prep(DF):\n",
    "    data = DF.filter(DF[\"text\"].isNull()==False).drop(\"_c0\")\n",
    "    noLinkText = data.withColumn('text_no_links',f.regexp_replace('text','http.*($|\\s)',''))\n",
    "    nlpPipeDF = pipeline.fit(noLinkText).transform(noLinkText).select(\"text\",\"finished_clean_lemma\")\n",
    "    return nlpPipeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Eval(mlPiped,time):\n",
    "    s3 = boto3.resource('s3')\n",
    "    ldaModel=mlPiped.stages[2]\n",
    "    ll = ldaModel.logLikelihood(LDA_DF)\n",
    "    lp = ldaModel.logPerplexity(LDA_DF)\n",
    "    with open('results.txt','w') as file:\n",
    "        file.write(\"Learned topics for \"+str(time)+\" congress tweets (as distributions over vocab of \"\n",
    "                    + str(ldaModel.vocabSize())+ \" words):\\n\\n\")\n",
    "        file.write(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "        file.write(\"\\nThe upper bound on perplexity: \" + str(lp))\n",
    "        file.write(\"\\n\")\n",
    "    #ldaModel.describeTopics(5).show(truncate=False)\n",
    "    # Output topics. Each is a distribution over words (matching word count vectors)\n",
    "    topics = ldaModel.describeTopics(40)\n",
    "    topics_rdd = topics.rdd\n",
    "    vocab = mlPiped.stages[0].vocabulary\n",
    "    topics_words = topics_rdd\\\n",
    "           .map(lambda row: row['termIndices'])\\\n",
    "           .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "           .collect()\n",
    "    with open('results.txt','a') as file:\n",
    "        for idx, topic in enumerate(topics_words):\n",
    "            file.write(\"\\n----------\")\n",
    "            file.write(\"\\ntopic: \"+str(idx))\n",
    "            file.write(\"\\n----------\\n\")\n",
    "            for word in topic:\n",
    "                file.write(word+\" | \")\n",
    "        file.write(\"\\n========================================================================\\n\")\n",
    "    s3.meta.client.upload_file('results.txt', '502finalprojbucky',\n",
    "                               'congressresults/results'+str(time).replace(\" \",\"\")+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Monthly2017 = [congDF0617,congDF0717,congDF0817,congDF0917,\n",
    "               congDF1117,congDF1117,congDF1217]\n",
    "Monthly17 = [\"June 2017\",\"July 2017\",\"Aug 2017\",\"Sep 2017\",\n",
    "             \"Oct 2017\",\"Nov 2017\",\"Dec 2017\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Monthly2018 = [congDF0118,congDF0218,congDF0318,\n",
    "               congDF0418,congDF0518,congDF0618,\n",
    "               congDF0718,congDF0818,congDF0918,\n",
    "               congDF1018,congDF1118,congDF1218]\n",
    "Monthly18 = [\"Jan 2018\",\"Feb 2018\",\"Mar 2018\",\"Apr 2018\",\n",
    "             \"May 2018\",\"June 2018\",\"July 2018\",\"Aug 2018\",\n",
    "             \"Sep 2018\",\"Oct 2018\",\"Nov 2018\",\"Dec 2018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Monthly2019 = [congDF0119,congDF0219,congDF0319,\n",
    "               congDF0419,congDF0519,congDF0619,\n",
    "               congDF0719,congDF0819,congDF0919,\n",
    "               congDF1019,congDF1119,congDF1219]\n",
    "Monthly19 = [\"Jan 2019\",\"Feb 2019\",\"Mar 2019\",\"Apr 2019\",\n",
    "             \"May 2019\",\"June 2019\",\"July 2019\",\"Aug 2019\",\n",
    "             \"Sep 2019\",\"Oct 2019\",\"Nov 2019\",\"Dec 2019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,month in zip(Monthly2017,Monthly17):\n",
    "    # Prep the data for LDA\n",
    "    prepDF = LDA_Prep(df)    \n",
    "    # Fit the model\n",
    "    mlPipeFit = mlPipeline.fit(prepDF)\n",
    "    # Transform the data\n",
    "    LDA_DF = mlPipeFit.transform(prepDF)\n",
    "    # Write summary of the results to a text file in s3 bucket\n",
    "    LDA_Eval(mlPipeFit,month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,month in zip(Monthly2018,Monthly18):\n",
    "    # Prep the data for LDA\n",
    "    prepDF = LDA_Prep(df)    \n",
    "    # Fit the model\n",
    "    mlPipeFit = mlPipeline.fit(prepDF)\n",
    "    # Transform the data\n",
    "    LDA_DF = mlPipeFit.transform(prepDF)\n",
    "    # Write summary of the results to a text file in s3 bucket\n",
    "    LDA_Eval(mlPipeFit,month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,month in zip(Monthly2019,Monthly19):\n",
    "    # Prep the data for LDA\n",
    "    prepDF = LDA_Prep(df)    \n",
    "    # Fit the model\n",
    "    mlPipeFit = mlPipeline.fit(prepDF)\n",
    "    # Transform the data\n",
    "    LDA_DF = mlPipeFit.transform(prepDF)\n",
    "    # Write summary of the results to a text file in s3 bucket\n",
    "    LDA_Eval(mlPipeFit,month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
