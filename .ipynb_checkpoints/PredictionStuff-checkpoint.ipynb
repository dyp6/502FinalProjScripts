{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp-prediction\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('rt')\n",
    "eng_stopwords.append('qt')\n",
    "eng_stopwords.append('&amp')\n",
    "eng_stopwords.append('amp')\n",
    "eng_stopwords.append('+')\n",
    "eng_stopwords.append('w')\n",
    "eng_stopwords.append('today')\n",
    "eng_stopwords.append('make')\n",
    "eng_stopwords.append('hear')\n",
    "eng_stopwords.append('meet')\n",
    "eng_stopwords.append('see')\n",
    "eng_stopwords.append('time')\n",
    "eng_stopwords.append('day')\n",
    "eng_stopwords.append('watch')\n",
    "eng_stopwords.append('get')\n",
    "eng_stopwords.append('im')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import pyspark.sql.functions as f\n",
    "from operator import itemgetter\n",
    "import pyspark.sql.types as T\n",
    "import boto3, os, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text_no_links') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Prep(DF):\n",
    "    noLinkText = DF.withColumn('text_no_links',f.regexp_replace('tweet_text','http.*($|\\s)',''))\n",
    "    nlpPipeDF = pipeline.fit(noLinkText).transform(noLinkText).select(\"tweet_text\",\"finished_clean_lemma\",\"tweet_type\")\n",
    "    return nlpPipeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse vector of counts for LDA input featuresCol\n",
    "cv = CountVectorizer(inputCol=\"finished_clean_lemma\",\n",
    "                     outputCol=\"features\", vocabSize=7500,\n",
    "                     minDF=15)\n",
    "# IDF to prepare for LDA\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"idf\")\n",
    "# LDA model\n",
    "lda = LDA(k=2, maxIter=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_Model = PipelineModel(stages=[cv,idf,lda]).load(\"s3://502finalprojbucky/FullModel/model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "InfoOp = spark.read.csv(\"s3://502finalprojbucky/InfOpEnglish/*.csv\").dropDuplicates().na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = spark.read.csv(\"s3://502finalprojbucky/YearlyCongress/*.csv\").dropDuplicates().na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "InfoOp=InfoOp.withColumn(\"tweet_type\",f.lit(\"InfoOp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = congress.withColumn(\"tweet_type\",f.lit(\"Congress\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "InfoOp = InfoOp.withColumnRenamed(\"_c3\",\"tweet_text\")\\\n",
    "                .select([\"tweet_text\",\"tweet_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = congress.withColumnRenamed(\"_c5\",\"tweet_text\")\\\n",
    "                .select([\"tweet_text\",\"tweet_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampleDF = sc.union([InfoOp.rdd,congress.rdd]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrepDF = LDA_Prep(SampleDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SampleTrans = LDA_Model.transform(PrepDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = SampleTrans.select([\"tweet_type\",\"topicDistribution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= preds.withColumn(\"idx\",f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = f.udf(ith_, T.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsDF = preds\\\n",
    "        .withColumn(\"T1\",ith(preds[\"topicDistribution\"], f.lit(0)))\\\n",
    "        .withColumn(\"T2\",ith(preds[\"topicDistribution\"], f.lit(1)))\\\n",
    "        .select([\"tweet_type\",\"T1\",\"T2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsDF = predsDF.where(predsDF.T1 != predsDF.T2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=T.StructType([T.StructField('maxval',T.IntegerType()),T.StructField('maxval_colname',T.StringType())])\n",
    "\n",
    "maxcol = f.udf(lambda row: max(row,key=itemgetter(0)), schema)\n",
    "maxDF = predsDF.withColumn('maxfield', maxcol(f.struct([f.struct(predsDF[x],f.lit(x)) for x in predsDF.columns[1:]]))).\\\n",
    "select(predsDF.columns+['maxfield.maxval_colname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "InfT1 = maxDF.where(maxDF.tweet_type==\"InfoOp\").where(maxDF.maxval_colname==\"T1\").count()\n",
    "InfT2 = maxDF.where(maxDF.tweet_type==\"InfoOp\").where(maxDF.maxval_colname==\"T2\").count()\n",
    "CongT1 = maxDF.where(maxDF.tweet_type==\"Congress\").where(maxDF.maxval_colname==\"T1\").count()\n",
    "CongT2 = maxDF.where(maxDF.tweet_type==\"Congress\").where(maxDF.maxval_colname==\"T2\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "InfPercSame = max([InfT1,InfT2])/sum([InfT1,InfT2])\n",
    "CongPercSame = max([CongT1,CongT2])/sum([CongT1,CongT2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of Congress Tweets in Same Topic: 0.7788784543128019\n",
      "Percent of Suspected Information Operations Tweets in Same Topic: 0.5663152325399131\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of Congress Tweets in Same Topic: \"+str(CongPercSame))\n",
    "print(\"Percent of Suspected Information Operations Tweets in Same Topic: \"+str(InfPercSame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congress Tweets in Topic 1: 296277\n",
      "Congress Tweets in Topic 2: 1043606\n"
     ]
    }
   ],
   "source": [
    "print(\"Congress Tweets in Topic 1: \"+str(CongT1))\n",
    "print(\"Congress Tweets in Topic 2: \"+str(CongT2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suspected Information Operations Tweets in Topic 1: 3320266\n",
      "Suspected Information Operations Tweets in Topic 2: 2542663\n"
     ]
    }
   ],
   "source": [
    "print(\"Suspected Information Operations Tweets in Topic 1: \"+str(InfT1))\n",
    "print(\"Suspected Information Operations Tweets in Topic 2: \"+str(InfT2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_Topics(mlPiped):\n",
    "    s3 = boto3.resource('s3')\n",
    "    ldaModel=mlPiped.stages[2]\n",
    "    with open('resultsfull.txt','w') as file:\n",
    "        file.write(\"Results for LDA Topic Prediction on Full Set of Tweets\\n\")\n",
    "        file.write(\"=========================================================================\\\n",
    "==============================================================================\\n\\n\")\n",
    "        file.write(\"Topic 1: \"+\n",
    "                   str(sum([CongT1,InfT1]))+\" Total Tweets\\n  - \"+\n",
    "                   str(InfT1)+\" Tweets from Suspected Information Operations\\n  - \"+\n",
    "                   str(CongT1)+\" Tweets from Congress Accounts\\n\\n\")\n",
    "        file.write(\"Topic 2: \"+\n",
    "                   str(sum([CongT2,InfT2]))+\" Total Tweets\\n  - \"+\n",
    "                   str(InfT2)+\" Tweets from Suspected Information Operations Tweets\\n  - \"+\n",
    "                   str(CongT2)+\" Tweets from Congress Accounts\\n\\n\")\n",
    "        file.write(\"Distribution of Tweet Type by Topic:\\n\")\n",
    "        file.write(\"  - Topic 1: \"+str(round(InfPercSame*100,2))+\n",
    "                   \"% of all Suspected Information Operations Tweets, and \"+\n",
    "                   str(round((1 - CongPercSame)*100,2))+\"% of all Congress Tweets\\n\")\n",
    "        file.write(\"  - Topic 2: \"+str(round((1-InfPercSame)*100,2))+\n",
    "                   \"% of all Suspected Information Operations Tweets, and \"+\n",
    "                   str(round(CongPercSame*100,2))+\"% of all Congress Tweets\\n\\n\\n\")\n",
    "    #ldaModel.describeTopics(5).show(truncate=False)\n",
    "    # Output topics. Each is a distribution over words (matching word count vectors)\n",
    "    topics = ldaModel.describeTopics(50)\n",
    "    topics_rdd = topics.rdd\n",
    "    vocab = mlPiped.stages[0].vocabulary\n",
    "    topics_words = topics_rdd\\\n",
    "           .map(lambda row: row['termIndices'])\\\n",
    "           .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "           .collect()\n",
    "    with open('resultsfull.txt','a') as file:\n",
    "        file.write(\"Top 50 Words for Both Topics\\n\")\n",
    "        file.write(\"=========================================================================\\\n",
    "==============================================================================\\n\")\n",
    "        for idx, topic in enumerate(topics_words):\n",
    "            file.write(\"\\nTopic: \"+str(idx+1))\n",
    "            file.write(\"\\n-----------------------------------------------------------------------\\\n",
    "-------------------------------------------------------------------------------\\n\")\n",
    "            for word in topic:\n",
    "                file.write(word+\" | \")\n",
    "            file.write(\"\\n\")\n",
    "    s3.meta.client.upload_file('resultsfull.txt', '502finalprojbucky',\n",
    "                               'FullResults/fullresults.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_Topics(LDA_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
