{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('rt')\n",
    "eng_stopwords.append('qt')\n",
    "eng_stopwords.append('&amp')\n",
    "eng_stopwords.append('amp')\n",
    "eng_stopwords.append('+')\n",
    "eng_stopwords.append('w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text_no_links') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, monotonically_increasing_id, col, when, arrays_zip, explode\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.clustering import LDA, LDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/June2017.csv/*.part\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = congDF.drop(\"_c0\")\n",
    "data = congDF.filter(congDF['text'].isNull()==False)\n",
    "\n",
    "#data.select('text').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noLinks = data.withColumn('index',monotonically_increasing_id())\n",
    "noLinks = noLinks.withColumn('text_no_links',regexp_replace('text','http.*($|\\s)',''))\n",
    "\n",
    "#noLinks.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text with the pipeline\n",
    "congress = pipeline.fit(noLinks).transform(noLinks)\n",
    "#congress.select('finished_clean_lemma').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = congress.select('finished_clean_lemma').withColumn('index',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "cv = CountVectorizer(inputCol=\"finished_clean_lemma\",outputCol=\"features\",\n",
    "                     vocabSize=3500,minDF = 8.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF\n",
    "cvmodel = cv.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "result_cv = cvmodel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=3\n",
    "max_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(k=num_topics,maxIter=max_iter,optimizer='online').fit(result_cv.select(\"index\",\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = lda_model.transform(result_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTopDist(row):\n",
    "    return row.topicDistribution.toArray().tolist()\n",
    "DF = transform.rdd.map(extractTopDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = spark.createDataFrame(DF,[\"Topic1\",\"Topic2\",\"Topic3\"]).withColumn(\"index_1\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoTopDF = transform.join(DF,transform.index==DF.index_1,'inner')\\\n",
    ".select([\"index\",\"finished_clean_lemma\",\"features\",\"Topic1\",\"Topic2\",\"Topic3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "CongressTopics = CoTopDF.rdd.map(lambda r: r.asDict())\\\n",
    "       .map(lambda r: Row(MaxTopic=[max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                        key=lambda kv: kv[1])[0],\n",
    "                                   max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                      key=lambda kv: kv[1])[1]], **r) )\\\n",
    "       .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsCongress = CongressTopics.withColumn(\"tweet_content\",col(\"finished_clean_lemma\"))\\\n",
    ".withColumn(\"Idx\",col(\"index\"))\\\n",
    ".drop(\"finished_clean_lemma\").drop(\"index\")\\\n",
    ".drop(\"Topic1\").drop(\"Topic2\").drop(\"Topic3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "|MaxTopic                    |features                                                                                                                      |tweet_content                                                                                                                                                           |Idx|\n",
      "+----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "|[Topic2, 0.9421104422984041]|(3196,[1,2,20,27,124,255,286,1709],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                         |[listen, story, join, fight, affordable, accessible, health, care]                                                                                                      |0  |\n",
      "|[Topic3, 0.9652194491446401]|(3196,[29,41,108,131,238,378,687,843,1197,1679,1700,2329,2425],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |[homelanddems, rank, member, benniegthompson, dhsgov, secretary, kelly, must, see, clear, guidance, scotus, rule, travelban, implementation]                            |1  |\n",
      "|[Topic2, 0.9207304102446401]|(3196,[75,91,171,230,354,472,494,893,1089,1157,1170,1225,1257,2455],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[ladepthealth, reprichmond, nwlc, aarplouisiana, nationalcouncil, senatehealthcarebill, substantially, increase, cost, la, private, insurance, shopper, much, old, folk]|2  |\n",
      "|[Topic3, 0.5196642055210542]|(3196,[6,14,17,131,344,369,475,637,829,1586],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |[victory, american, people, especially, farmer, forester, southwest, alabama, would, suffer, rule, wotus]                                                               |3  |\n",
      "|[Topic3, 0.6173741857528392]|(3196,[120,124,199,443,813,1976],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                   |[realdonaldtrump, wow, cnn, retract, big, story, russia]                                                                                                                |4  |\n",
      "|[Topic2, 0.8503999620636378]|(3196,[0,3,5,10,79,99,172,177,362,365,539],[1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0])                                     |[tell, press, house, senate, trumpcare, bill, mean, mean, bad, policy, bad, process, bad, politics]                                                                     |5  |\n",
      "|[Topic2, 0.5856458333594142]|(3196,[11,24,28,76,644,812,1345,1520],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |[republican, dream, day, get, slash, medicaid, might, achieve]                                                                                                          |6  |\n",
      "|[Topic3, 0.9360133729489128]|(3196,[143,344,395,498,661,683,1707],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                           |[wotus, gross, overreach, obama, admin, put, mud, puddle, backyard, ditch, government, control]                                                                         |7  |\n",
      "|[Topic2, 0.6782573508721985]|(3196,[10,20,24,32,88,248,276],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])                                                                 |[keithrschmidt, reppaultonko, fight, begin, house, pass, let, get, complacent, fight, isnt]                                                                             |8  |\n",
      "|[Topic3, 0.953244815762649] |(3196,[10,108,187,317,378,782,1149,1384,1529,2547],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                 |[icemarkets, clear, house, risk, management, practice, prove, resolve, member, default, include, large, bankruptcy, proceedings]                                        |9  |\n",
      "|[Topic2, 0.9661397741956418]|(3196,[3,11,19,26,87,93,124,149,317,546,551,644,752],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |[instead, help, struggle, w, addiction, trumpcare, slash, medicaid, fund, nathans, story, show, risk, protectourcare]                                                   |10 |\n",
      "|[Topic2, 0.5617243182992261]|(3196,[0,15,88,104,146,188],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                        |[great, news, cant, let, bill, still]                                                                                                                                   |11 |\n",
      "|[Topic2, 0.971189051083999] |(3196,[0,3,5,8,12,14,43,80,124,140,146,181,203,230,280,470],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[senate, delay, vote, trumpcare, heartless, bill, would, hurt, millions, still, much, work, please, keep, share, story]                                                 |12 |\n",
      "|[Topic2, 0.9401386209247392]|(3196,[3,79,120,201,301,470,829],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                               |[sure, realdonaldtrump, senior, trumpcare, especially, mean, heartless, costly]                                                                                         |13 |\n",
      "|[Topic2, 0.649639326119213] |(3196,[0,16,24,25,46,92,145,197,233,242,308,539,1187],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                  |[proud, support, bill, long, allow, politics, get, way, give, troop, raise, need, deserve]                                                                              |14 |\n",
      "|[Topic1, 0.431394685964485] |(3196,[0,9,56,186,286,343,365,618,1026,1545,1980],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                              |[chcbvt, thank, senatorleahy, host, press, conference, chcb, yesterday, pleasure, speak, affordable, rx, bill]                                                          |15 |\n",
      "|[Topic2, 0.9468522332981629]|(3196,[62,91,242,816,1234,1560,1947],[1.0,1.0,1.0,2.0,1.0,1.0,1.0])                                                           |[fightfor, truth, minimum, wage, seattle, raise, wage, cost, job]                                                                                                       |16 |\n",
      "|[Topic3, 0.948349005052682] |(3196,[22,32,46,51,65,94,114,128,211],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                  |[im, proud, protection, child, act, pass, committee, last, week]                                                                                                        |17 |\n",
      "|[Topic3, 0.943083658160025] |(3196,[33,150,575,672,703,729,740,1493],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                    |[ambassador, nikkihaley, already, take, significant, step, restore, sanity, un]                                                                                         |18 |\n",
      "|[Topic1, 0.6879160715791691]|(3196,[9,17,27,139,621,697,731],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                |[cffoundation, thank, join, congressional, cf, caucus, repgonzalez, appreciate, people, cysticfibrosis, cfadvocacy]                                                     |19 |\n",
      "+----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TopicsCongress.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvRDD = result_cv.select(\"features\",\"finished_clean_lemma\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "termIdxRDD = cvRDD.map(lambda r: Row(\\\n",
    "                                     Idx=[int(str(r[0].indices[i])) for i in range(len(r[0].indices))],\n",
    "                                     Term=[r[1][i] for i in range(len(r[0].indices))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PairedRDD = termIdxRDD.toDF().withColumn(\"tmp\",arrays_zip(\"Idx\",\"Term\")).withColumn(\"IdxPairs\",explode(\"tmp\")).select(\"IdxPairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "PairedDF = PairedRDD.rdd.map(lambda r: Row(Index=r[0][0],\n",
    "                              Term = r[0][1])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
