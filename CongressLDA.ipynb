{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('rt')\n",
    "eng_stopwords.append('qt')\n",
    "eng_stopwords.append('&amp')\n",
    "eng_stopwords.append('amp')\n",
    "eng_stopwords.append('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text_no_links') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, monotonically_increasing_id, col, when\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.clustering import LDA, LDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/June2017.csv/*.part\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = congDF.drop(\"_c0\")\n",
    "data = congDF.filter(congDF['text'].isNull()==False)\n",
    "\n",
    "#data.select('text').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noLinks = data.withColumn('index',monotonically_increasing_id())\n",
    "noLinks = noLinks.withColumn('text_no_links',regexp_replace('text','http.*($|\\s)',''))\n",
    "\n",
    "#noLinks.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text with the pipeline\n",
    "congress = pipeline.fit(noLinks).transform(noLinks)\n",
    "#congress.select('finished_clean_lemma').show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = congress.select('finished_clean_lemma').withColumn('index',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "cv = CountVectorizer(inputCol=\"finished_clean_lemma\",outputCol=\"features\",\n",
    "                     vocabSize=3500,minDF = 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF\n",
    "cvmodel = cv.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "result_cv = cvmodel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=3\n",
    "max_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(k=num_topics,maxIter=max_iter,optimizer='online').fit(result_cv.select(\"index\",\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = lda_model.transform(result_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(row):\n",
    "    return row.topicDistribution.toArray().tolist()\n",
    "DF = transform.rdd.map(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = spark.createDataFrame(DF,[\"Topic1\",\"Topic2\",\"Topic3\"]).withColumn(\"index_1\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoTopDF = transform.join(DF,transform.index==DF.index_1,'inner')\\\n",
    ".select([\"index\",\"finished_clean_lemma\",\"features\",\"Topic1\",\"Topic2\",\"Topic3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "CongressTopics = CoTopDF.rdd.map(lambda r: r.asDict())\\\n",
    "       .map(lambda r: Row(MaxTopic=[max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                        key=lambda kv: kv[1])[0],\n",
    "                                   max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                      key=lambda kv: kv[1])[1]], **r) )\\\n",
    "       .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsCongress = CongressTopics.withColumn(\"tweet_content\",col(\"finished_clean_lemma\"))\\\n",
    ".withColumn(\"Idx\",col(\"index\"))\\\n",
    ".drop(\"finished_clean_lemma\").drop(\"index\")\\\n",
    ".drop(\"Topic1\").drop(\"Topic2\").drop(\"Topic3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "|MaxTopic                    |features                                                                                                                      |tweet_content                                                                                                                                                           |Idx|\n",
      "+----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "|[Topic1, 0.8549260747134776]|(3196,[1,2,20,27,124,255,283,1661],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                         |[listen, story, join, fight, affordable, accessible, health, care]                                                                                                      |0  |\n",
      "|[Topic2, 0.5302429467184558]|(3196,[29,41,108,131,237,377,676,834,1176,1674,1705,2285,2399],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |[homelanddems, rank, member, benniegthompson, dhsgov, secretary, kelly, must, see, clear, guidance, scotus, rule, travelban, implementation]                            |1  |\n",
      "|[Topic3, 0.8232240284592021]|(3196,[75,91,171,229,356,471,494,900,1095,1157,1165,1222,1254,2402],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[ladepthealth, reprichmond, nwlc, aarplouisiana, nationalcouncil, senatehealthcarebill, substantially, increase, cost, la, private, insurance, shopper, much, old, folk]|2  |\n",
      "|[Topic3, 0.5758709339627895]|(3196,[6,14,17,131,346,368,476,635,828,1597],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |[victory, american, people, especially, farmer, forester, southwest, alabama, would, suffer, rule, wotus]                                                               |3  |\n",
      "|[Topic2, 0.4783671367475659]|(3196,[120,124,200,439,807,1932],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                   |[realdonaldtrump, wow, cnn, retract, big, story, russia]                                                                                                                |4  |\n",
      "|[Topic3, 0.9650444517197158]|(3196,[0,3,5,10,79,101,172,177,362,364,539],[1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0])                                    |[tell, press, house, senate, trumpcare, bill, mean, mean, bad, policy, bad, process, bad, politics]                                                                     |5  |\n",
      "|[Topic3, 0.7075764381024068]|(3196,[11,24,28,76,639,814,1351,1530],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |[republican, dream, day, get, slash, medicaid, might, achieve]                                                                                                          |6  |\n",
      "|[Topic2, 0.7722144473201474]|(3196,[143,346,398,497,656,682,1688],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                           |[wotus, gross, overreach, obama, admin, put, mud, puddle, backyard, ditch, government, control]                                                                         |7  |\n",
      "|[Topic1, 0.5880814271834642]|(3196,[10,20,24,32,89,247,277],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])                                                                 |[keithrschmidt, reppaultonko, fight, begin, house, pass, let, get, complacent, fight, isnt]                                                                             |8  |\n",
      "|[Topic3, 0.7693407491724942]|(3196,[10,108,187,317,377,778,1169,1407,1532,2549],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                 |[icemarkets, clear, house, risk, management, practice, prove, resolve, member, default, include, large, bankruptcy, proceedings]                                        |9  |\n",
      "|[Topic3, 0.8542210139602776]|(3196,[3,11,19,26,87,93,124,149,317,545,553,639,753],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                   |[instead, help, struggle, w, addiction, trumpcare, slash, medicaid, fund, nathans, story, show, risk, protectourcare]                                                   |10 |\n",
      "|[Topic1, 0.6714987770085145]|(3196,[0,15,89,104,146,190],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                        |[great, news, cant, let, bill, still]                                                                                                                                   |11 |\n",
      "|[Topic3, 0.6622889806901299]|(3196,[0,3,5,8,12,14,43,80,124,140,146,180,204,229,280,467],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[senate, delay, vote, trumpcare, heartless, bill, would, hurt, millions, still, much, work, please, keep, share, story]                                                 |12 |\n",
      "|[Topic3, 0.9363986045347757]|(3196,[3,79,120,201,301,467,828],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                               |[sure, realdonaldtrump, senior, trumpcare, especially, mean, heartless, costly]                                                                                         |13 |\n",
      "|[Topic3, 0.9606208955828474]|(3196,[0,16,24,25,46,92,144,197,233,244,308,539,1190],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                  |[proud, support, bill, long, allow, politics, get, way, give, troop, raise, need, deserve]                                                                              |14 |\n",
      "|[Topic2, 0.6682746652899801]|(3196,[0,9,56,186,283,344,364,622,1020,1556,1947],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                              |[chcbvt, thank, senatorleahy, host, press, conference, chcb, yesterday, pleasure, speak, affordable, rx, bill]                                                          |15 |\n",
      "|[Topic3, 0.5418727979047995]|(3196,[62,91,244,821,1205,1584,1943],[1.0,1.0,1.0,2.0,1.0,1.0,1.0])                                                           |[fightfor, truth, minimum, wage, seattle, raise, wage, cost, job]                                                                                                       |16 |\n",
      "|[Topic2, 0.5629891057800513]|(3196,[22,32,46,51,65,95,114,128,211],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                  |[im, proud, protection, child, act, pass, committee, last, week]                                                                                                        |17 |\n",
      "|[Topic2, 0.6857094892842184]|(3196,[33,150,578,672,703,733,740,1488],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                    |[ambassador, nikkihaley, already, take, significant, step, restore, sanity, un]                                                                                         |18 |\n",
      "|[Topic2, 0.6664884636023305]|(3196,[9,17,27,139,615,688,734],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                |[cffoundation, thank, join, congressional, cf, caucus, repgonzalez, appreciate, people, cysticfibrosis, cfadvocacy]                                                     |19 |\n",
      "+----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TopicsCongress.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
