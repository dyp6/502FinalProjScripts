{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-nlp\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('rt')\n",
    "eng_stopwords.append('qt')\n",
    "eng_stopwords.append('&amp')\n",
    "eng_stopwords.append('amp')\n",
    "eng_stopwords.append('+')\n",
    "eng_stopwords.append('w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text_no_links') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        normalizer,\n",
    "        lemmatizer,\n",
    "        stopwords_cleaner,\n",
    "        finisher\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, monotonically_increasing_id, col, when, arrays_zip, explode\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.clustering import LDA, LDAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = spark.read.csv(\"s3://502finalprojbucky/congresstweets/data/June2017.csv/*.part\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "congDF = congDF.drop(\"_c0\")\n",
    "data = congDF.filter(congDF['text'].isNull()==False)\n",
    "\n",
    "#data.select('text').show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noLinks = data.withColumn('text_no_links',regexp_replace('text','http.*($|\\s)',''))\n",
    "\n",
    "#noLinks.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform text with the pipeline\n",
    "congress = pipeline.fit(noLinks).transform(noLinks).withColumn('index',monotonically_increasing_id())\n",
    "#congress.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = congress.select('finished_clean_lemma').withColumn('index',monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF\n",
    "cv = CountVectorizer(inputCol=\"finished_clean_lemma\",outputCol=\"features\",\n",
    "                     vocabSize=3500,minDF = 8.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF\n",
    "cvmodel = cv.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "result_cv = cvmodel.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenIds = result_cv.select(\"features\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts out of here too\n",
    "vocabKeys = tokenIds.map(lambda r: [r[0].indices.tolist()]).toDF([\"termIdx\"]).withColumn(\"idx\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabArr = cvmodel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordRDD = result_cv.select(\"finished_clean_lemma\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = wordRDD.map(lambda r: [[i for i in r[0] if i in vocabArr]]).toDF([\"terms\"]).withColumn(\"idx1\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabMap = vocabKeys.join(words,vocabKeys.idx==words.idx1).drop(\"idx\").drop(\"idx1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=3\n",
    "max_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(k=num_topics,maxIter=max_iter,optimizer='online').fit(result_cv.select(\"index\",\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = lda_model.transform(result_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTopDist(row):\n",
    "    return row.topicDistribution.toArray().tolist()\n",
    "DF = transform.rdd.map(extractTopDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = spark.createDataFrame(DF,[\"Topic1\",\"Topic2\",\"Topic3\"]).withColumn(\"index_1\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoTopDF = transform.join(DF,transform.index==DF.index_1,'inner')\\\n",
    ".select([\"index\",\"finished_clean_lemma\",\"features\",\"Topic1\",\"Topic2\",\"Topic3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CongressTopics = CoTopDF.rdd.map(lambda r: r.asDict())\\\n",
    "       .map(lambda r: Row(MaxTopic=[max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                        key=lambda kv: kv[1])[0],\n",
    "                                   max([i for i in r.items() if i[0]\\\n",
    "                                        not in [\"index\",\"finished_clean_lemma\",\"features\"]], \n",
    "                                      key=lambda kv: kv[1])[1]], **r) )\\\n",
    "       .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicsCongress = CongressTopics.withColumn(\"tweet_content\",col(\"finished_clean_lemma\"))\\\n",
    ".withColumn(\"Idx\",col(\"index\"))\\\n",
    ".drop(\"finished_clean_lemma\").drop(\"index\")\\\n",
    ".drop(\"Topic1\").drop(\"Topic2\").drop(\"Topic3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "|MaxTopic                     |features                                                                                                                      |tweet_content                                                                                                                                                           |Idx|\n",
      "+-----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "|[Topic3, 0.6557127584670329] |(3195,[1,2,19,26,123,254,284,1688],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                         |[listen, story, join, fight, affordable, accessible, health, care]                                                                                                      |0  |\n",
      "|[Topic1, 0.9695563236452512] |(3195,[28,40,107,130,237,378,680,845,1181,1697,1699,2273,2440],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |[homelanddems, rank, member, benniegthompson, dhsgov, secretary, kelly, must, see, clear, guidance, scotus, rule, travelban, implementation]                            |1  |\n",
      "|[Topic3, 0.9636390416763602] |(3195,[74,90,170,228,356,472,493,888,1092,1152,1159,1230,1257,2481],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[ladepthealth, reprichmond, nwlc, aarplouisiana, nationalcouncil, senatehealthcarebill, substantially, increase, cost, la, private, insurance, shopper, much, old, folk]|2  |\n",
      "|[Topic1, 0.5637654961264718] |(3195,[6,14,17,130,342,368,479,630,824,1583],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                       |[victory, american, people, especially, farmer, forester, southwest, alabama, would, suffer, rule, wotus]                                                               |3  |\n",
      "|[Topic2, 0.7604669375265137] |(3195,[119,123,198,440,803,2001],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                   |[realdonaldtrump, wow, cnn, retract, big, story, russia]                                                                                                                |4  |\n",
      "|[Topic3, 0.8540322233328809] |(3195,[0,3,5,10,78,99,171,176,359,363,535],[1.0,1.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0])                                     |[tell, press, house, senate, trumpcare, bill, mean, mean, bad, policy, bad, process, bad, politics]                                                                     |5  |\n",
      "|[Topic3, 0.6813784654053301] |(3195,[11,23,27,75,641,808,1352,1520],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                      |[republican, dream, day, get, slash, medicaid, might, achieve]                                                                                                          |6  |\n",
      "|[Topic1, 0.9453437703861763] |(3195,[142,342,399,498,654,679,1666],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                           |[wotus, gross, overreach, obama, admin, put, mud, puddle, backyard, ditch, government, control]                                                                         |7  |\n",
      "|[Topic3, 0.6127025552120885] |(3195,[10,19,23,31,87,246,274],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])                                                                 |[keithrschmidt, reppaultonko, fight, begin, house, pass, let, get, complacent, fight, isnt]                                                                             |8  |\n",
      "|[Topic1, 0.959217898419726]  |(3195,[10,107,186,317,378,777,1158,1402,1501,2575],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                 |[icemarkets, clear, house, risk, management, practice, prove, resolve, member, default, include, large, bankruptcy, proceedings]                                        |9  |\n",
      "|[Topic3, 0.9568849920062226] |(3195,[3,11,25,86,92,123,148,317,544,549,641,749],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                          |[instead, help, struggle, addiction, trumpcare, slash, medicaid, fund, nathans, story, show, risk, protectourcare]                                                      |10 |\n",
      "|[Topic1, 0.5987484799849558] |(3195,[0,15,87,102,145,189],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                        |[great, news, cant, let, bill, still]                                                                                                                                   |11 |\n",
      "|[Topic3, 0.963277215327102]  |(3195,[0,3,5,8,12,14,42,79,123,139,145,179,202,228,280,468],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[senate, delay, vote, trumpcare, heartless, bill, would, hurt, millions, still, much, work, please, keep, share, story]                                                 |12 |\n",
      "|[Topic3, 0.9308893017778767] |(3195,[3,78,119,200,298,468,824],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                               |[sure, realdonaldtrump, senior, trumpcare, especially, mean, heartless, costly]                                                                                         |13 |\n",
      "|[Topic3, 0.578503897469185]  |(3195,[0,16,23,24,45,91,143,196,231,242,307,535,1174],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                  |[proud, support, bill, long, allow, politics, get, way, give, troop, raise, need, deserve]                                                                              |14 |\n",
      "|[Topic1, 0.43647630611456617]|(3195,[0,9,55,185,284,343,363,621,1036,1550,1932],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                              |[chcbvt, thank, senatorleahy, host, press, conference, chcb, yesterday, pleasure, speak, affordable, rx, bill]                                                          |15 |\n",
      "|[Topic3, 0.9383167728153259] |(3195,[61,90,242,823,1222,1605,1946],[1.0,1.0,1.0,2.0,1.0,1.0,1.0])                                                           |[fightfor, truth, minimum, wage, seattle, raise, wage, cost, job]                                                                                                       |16 |\n",
      "|[Topic1, 0.9553978140796021] |(3195,[21,31,45,50,64,94,113,128,210],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                  |[im, proud, protection, child, act, pass, committee, last, week]                                                                                                        |17 |\n",
      "|[Topic1, 0.8640583123914989] |(3195,[32,150,580,664,699,728,739,1479],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                    |[ambassador, nikkihaley, already, take, significant, step, restore, sanity, un]                                                                                         |18 |\n",
      "|[Topic2, 0.5852595078016403] |(3195,[9,17,26,138,616,698,731],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                |[cffoundation, thank, join, congressional, cf, caucus, repgonzalez, appreciate, people, cysticfibrosis, cfadvocacy]                                                     |19 |\n",
      "+-----------------------------+------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TopicsCongress.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvRDD = result_cv.select(\"features\",\"finished_clean_lemma\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "termIdxRDD = cvRDD.map(lambda r: Row(\\\n",
    "                                     Idx=[int(str(r[0].indices[i])) for i in range(len(r[0].indices))],\n",
    "                                     Term=[r[1][i] for i in range(len(r[0].indices))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PairedRDD = termIdxRDD.toDF().withColumn(\"tmp\",arrays_zip(\"Idx\",\"Term\")).withColumn(\"IdxPairs\",explode(\"tmp\")).select(\"IdxPairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PairedDF = PairedRDD.rdd.map(lambda r: Row(Index=r[0][0],\n",
    "                              Term = r[0][1])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
